---
title: 简介
comment: false
editLink: false
prev: false
next: false
order: 1
---


## 简介

LoRA（Low-Rank Adaptation）解决了在机器学习领域，特别是微调大型语言模型时遇到的一个重大挑战。传统的微调方法需要大量的计算资源和存储空间，这对于许多用户来说是不可承受的。LoRA通过显著减少可训练参数的数量，使微调过程更加高效和可行。


LoRA通过将权重更新分解为低秩矩阵来实现这一点。与其在微调过程中更新整个权重矩阵，LoRA只更新一小部分低秩矩阵。这种方法大大减少了需要训练的参数数量，从而降低了计算成本并加快了训练时间。尽管减少了可训练参数，使用LoRA微调的模型性能与使用传统方法微调的模型相当。


总之，LoRA通过引入一种更高效的方法来减少可训练参数的数量，解决了资源密集型的大型语言模型微调问题。这一创新不仅降低了计算和存储需求，还保持了微调模型的性能，使其成为机器学习社区中一个有价值的工具。


## 参考

* [Using LoRA for Efficient Stable Diffusion Fine-Tuning](https://huggingface.co/blog/lora)
